<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhongkai Shangguan</title>

  <meta name="author" content="Zhongkai Shangguan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="6qogr8z7RZgR_SCX2GSXZNaoOxokGl59tYYreLe6uEg" />
  <meta property="og:image" content="images/zhongkai.jfif" />
  <meta name="og:title" content="Zhongkai Shangguan" />
  <meta name="title" content="Zhongkai Shangguan" />
  <link rel=" stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>


<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Zhongkai Shangguan</name>
                  <p>I am a fourth-year PhD student at Boston University, advised by Prof. <a
                      href="https://eshed1.github.io/">Eshed Ohn-Bar</a>.
                    
                  <p>

                  <p style="text-align:center">
                    <a href="mailto:sgzk@bu.edu">Email</a> &nbsp/&nbsp
                    <a href="https://github.com/LeonShangguan/">Github</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=5LiWfk4AAAAJ&hl=en">Google Scholar</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/zhongkai.png"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/zhongkai.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>




          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>My research interests lie in multimodal machine learning with its applications in assistive systems.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>




              <tr onmouseout="iccv_2023_stop()" onmouseover="iccv_2023_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='iccv_2023_video'><video width=100% muted autoplay loop>
                        <source src="images/xvo_seq.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='iccv_2023_still'><img src='images/xvo.png' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function iccv_2023_start() {
                      document.getElementById('iccv_2023_video').style.display = 'inline';
                      document.getElementById('iccv_2023_still').style.display = 'none';
                    }

                    function iccv_2023_stop() {
                      document.getElementById('iccv_2023_video').style.display = 'none';
                      document.getElementById('iccv_2023_still').style.display = 'inline';
                    }
                    iccv_2023_stop()
                  </script>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2309.16772.pdf">
                    <papertitle>XVO: Generalized Visual Odometry via Cross-Modal Self-Training
                      </papertitle>
                  </a>
                  <br>
                  Lei Lai*, <strong>Zhongkai Shangguan*</strong>, Jimuyang Zhang,
                   <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a>
                  <br>
                  <em>International Conference on Computer Vision (ICCV)</em>, 2023
                  <br>
                  <a href="https://arxiv.org/pdf/2309.16772.pdf">paper</a> &nbsp/&nbsp
                  <a href="https://genxvo.github.io/">webpage</a>
                  <p></p>
                  <p>We propose XVO, a semi-supervised learning method for training generalized monocular Visual Odometry (VO) models with 
                    robust off-the-shelf operation across diverse datasets and settings. We empirically demonstrate the benefits of semi-supervised 
                    training for learning a general-purpose direct VO regression network. Moreover, we demonstrate multi-modal supervision, including 
                    segmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate generalized representations for the VO task.
                  </p>
                </td>
              </tr>
              <!-- END PAPER XVO -->
              
              
              
              
              <tr onmouseout="eccv_2022_stop()" onmouseover="eccv_2022_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='eccv_2022_video'><video width=100% muted autoplay loop>
                        <source src="images/assiter.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <div id='eccv_2022_still'><img src='images/simulation.PNG' width=100%></div>
                  </div>
                  <script type="text/javascript">
                    function eccv_2022_start() {
                      document.getElementById('eccv_2022_video').style.display = 'inline';
                      document.getElementById('eccv_2022_still').style.display = 'none';
                    }

                    function eccv_2022_stop() {
                      document.getElementById('eccv_2022_video').style.display = 'none';
                      document.getElementById('eccv_2022_still').style.display = 'inline';
                    }
                    eccv_2022_stop()
                  </script>
                </td>



                <td
                  style="padding-left:20px;padding-right: 20px; padding-top:10px;padding-bottom: 10px;width:75%;vertical-align:middle">
                  <a href="https://eshed1.github.io/papers/assister_eccv2022.pdf">
                    <papertitle>ASSISTER: Assistive Navigation via Conditional Instruction Generation
                      </papertitle>
                  </a>
                  <br>
                  Zanming Huang*, <strong>Zhongkai Shangguan*</strong>, Jimuyang Zhang, Gilad Bar, <a href="https://www.linkedin.com/in/mattcboyd/">Matthew Boyd</a>,
                   <a href="https://scholar.google.com/citations?user=p9zVBV4AAAAJ&hl=en&oi=ao">Eshed Ohn-Bar</a>
                  <br>
                  <em>European Conference on Computer Vision (ECCV)</em>, 2022
                  <br>
                  <a href="https://eshed1.github.io/papers/assister_eccv2022.pdf">paper</a> &nbsp/&nbsp
                  <a href="https://github.com/h2xlab/ASSISTER">data and code</a>
                  <p></p>
                  <p>We introduce a novel vision-and-language navigation (VLN) task of learning to provide real-time guidance to a blind follower situated 
                    in complex dynamic navigation scenarios. We collect a multi-modal real-world benchmark with in-situ Orientation and Mobility (O&M) 
                    instructional guidance. We leverage the real-world study to inform the design of a larger-scale simulation benchmark. In the end, we present ASSISTER, an imitation-learned agent that can 
                    embody such effective guidance.
                  </p>
                </td>
              </tr>
              <!-- END PAPER Assister -->
              
              
              




           






          
              
              
              
              
              











            </tbody>
          </table>








          <!-- Teaching -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Teaching</heading>

                  <p>
                    ENG EC 500 - Robot Learning and Vision for Navigation - Spring 2023</a>
                    <br>
                    Teaching Assistant
                  </p>

                  <p>
                    ENG EC 518 - Robot Learning - Fall 2024</a>
                    <br>
                    Teaching Assistant
                  </p>

                  <p>
                    ENG EC 530 - Software Engineering Principles - Spring 2024</a>
                    <br>
                    Teaching Assistant
                  </p>
                  
                  

                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


            </tbody>
          </table>



          <!-- Service -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Service</heading>

                  <p>
                    <a href="https://accessibility-cv.github.io/">CVPR2022 AVA Accessibility Vision and Autonomy Challenge 2022, 2023, 2024</a>
                    <br>
                    Challenge Organizer
                  </p>

                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


            </tbody>
          </table>










          <tables
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    This website was forked from <a href="https://github.com/jonbarron/jonbarron_website">source
                      code</a>

                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>




